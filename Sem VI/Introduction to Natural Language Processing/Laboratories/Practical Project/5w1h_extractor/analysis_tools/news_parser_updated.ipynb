{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53ad70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd43236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf3b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(url):\n",
    "    headers = {'User-agent': 'Chrome/108.0.0.0'} \n",
    "    request = requests.get(url, headers=headers) \n",
    "    html = request.content\n",
    "    parser = BeautifulSoup(html, 'html.parser') \n",
    "    \n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9aced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2015/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f47a852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "CPU times: user 1min 44s, sys: 3.7 s, total: 1min 47s\n",
      "Wall time: 11min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2015/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2015/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "news_df.to_parquet('datasets/df_finante_pers_2015.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eaeb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n"
     ]
    }
   ],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2014/01/')\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2014/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2014/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "news_df.to_parquet('datasets/df_finante_pers_2014.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "432859e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n"
     ]
    }
   ],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2013/01/')\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2013/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/finante-personale/arhiva/2013/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "news_df.to_parquet('datasets/df_finante_pers_2013.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "934b7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n"
     ]
    }
   ],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2015/01/')\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2015/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2015/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "news_df.to_parquet('datasets/df_burse_fonduri_mutuale_2015.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db9851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n"
     ]
    }
   ],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2014/01/')\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2014/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2014/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "news_df.to_parquet('datasets/df_burse_fonduri_mutuale_2014.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8a79194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on month: 01\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 02\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 03\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 04\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 05\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 06\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 07\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 08\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 09\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 10\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 11\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n",
      "Working on month: 12\n",
      "Working on page: 1\n",
      "Working on page: 2\n",
      "Working on page: 3\n",
      "Working on page: 4\n",
      "Working on page: 5\n"
     ]
    }
   ],
   "source": [
    "init_parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2013/01/')\n",
    "#21:08 start\n",
    "news_df = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "#months = ['04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for month in months:\n",
    "    print('Working on month: {}'.format(month))\n",
    "    init_parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2013/' + month + '/')\n",
    "    max_page = 1\n",
    "    for parsed in init_parser.find_all(\"li\"):\n",
    "        if 'title' in parsed.find_next('a').attrs:\n",
    "            if 'pagina' in parsed.find_next('a')['title']:\n",
    "                if int(parsed.find_next('a')['title'][-1:]) > max_page:\n",
    "                    max_page = int(parsed.find_next('a')['title'][-1:])\n",
    "    for i in range(1, max_page+1):\n",
    "        print('Working on page: {}'.format(i))\n",
    "        \n",
    "        parser = create_parser('https://www.zf.ro/burse-fonduri-mutuale/arhiva/2013/'+ month + '/page/' + str(i))\n",
    "        \n",
    "        for article_h2 in parser.findAll('h2'):\n",
    "            href = article_h2.find_next('a', href=True)['href']\n",
    "            title = article_h2.find_next('a', href=True)\\\n",
    "                                          .contents[0]\\\n",
    "                                          .get_text()\\\n",
    "                                          .strip()\n",
    "            try:\n",
    "                parser_article = create_parser('https://www.zf.ro' + article_h2.find_next('a', href=True)['href'])\n",
    "\n",
    "                json_scr = json.loads(parser_article.find_all('script')[2].text, strict = False)\n",
    "                date_published = json_scr['datePublished']\n",
    "                headline = json_scr['headline']\n",
    "                article_section = json_scr['articleSection']\n",
    "                description = json_scr['description']\n",
    "                if 'name' in json_scr['author']:\n",
    "                    author_name = json_scr['author']['name']\n",
    "                else:\n",
    "                    author_name = None\n",
    "                article_text = json_scr['articleBody']\n",
    "                rec_refs = parser_article.find_all(\"a\", {\"class\": \"thumb picture-type\"})\n",
    "                rec_hrefs = []\n",
    "                for ref in range(len(rec_refs)):\n",
    "                    rec_hrefs.append(rec_refs[ref]['href'])\n",
    "\n",
    "                dict_append = {'href': href,\n",
    "                              'title': title,\n",
    "                              'date_published': date_published,\n",
    "                              'headline': headline,\n",
    "                              'article_section': article_section,\n",
    "                              'description': description,\n",
    "                              'author_name': author_name,\n",
    "                              'article_text': article_text,\n",
    "                              'rec_hrefs': rec_hrefs}\n",
    "                news_df = news_df.append(dict_append, ignore_index = True)\n",
    "            except:\n",
    "                continue\n",
    "news_df.to_parquet('datasets/df_burse_fonduri_mutuale_2013.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b90fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('datasets/df_finante_pers_2022.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6dd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
